# Tweet Sentiment Extraction


*Леппик Инрэ*

## Постановка задачи

Задача следующая -  на основе твита и его заданного настроения выделить ту часть твита, которая лучше всего отражает это настроение.
Выполнение такой задачи может быть полезно, так как она делает анализ тональности более интерпретируемым.
Задача была взята из следующего соревнования:
- [Ссылка](https://www.kaggle.com/competitions/tweet-sentiment-extraction/)

### Формат входных и выходных данных

На вход подается две строки: текст твита и его заданную тональность. На выходе система выдает текстовый фрагмент, который отражает это настроение.

### Метрики

Основная метрика: Jaccard Similarity вычисленная по истинным и предсказанным фрагментам твита. Ожидаю, что простейший бейзлайн даст порядка 0.45–0.55, а основная модель на базе трансформера около 0.65–0.70.

### Валидация и тест

Для валидации набор данных будет поделен в соотношении 80/20. При этом, чтобы сделать деление воспроизводимым, я зафиксирую seed = 42.

### Датасеты

Набор данных будет сформирован из следующих датасетов:

- [Основной датасет из соревнования](https://www.kaggle.com/competitions/tweet-sentiment-extraction/data?select=train.csv)
- [Пользовательский датасет с Kaggle для такой же задачи](https://www.kaggle.com/datasets/maxjon/complete-tweet-sentiment-extraction-data)

Всего в данных 3 класса (positive, negative, neutral). Первый датасет содержит в себе 39556 твитов, а второй содержит в себе 27481 твитов с выделенными частями, которые наиболее хорошо отражают настроение самого твита.
Для тренировки + валидации и теста данные делятся в соотношении 80/20, далее данные для тренировки + валидации также делятся в соотношении 80/20.

## Моделирование

### Бейзлайн

В качестве бейзлайн решения будет взята эвристика. Каждый текст будет токенизирован, а затем будет взята случайная последовательности из 1-5 токенов (в зависимости от размера текста) в качестве выхода.

### Основная модель
В качестве основной модели будет использоваться модель основанная на архитектуре RoBERTa, которая была использована на соревновании. 

- [Ссылка на ноутбук взятый за основу](https://www.kaggle.com/code/abhishek/roberta-on-steroids-pytorch-tpu-training/notebook)
  
Количество параметров и размер будут позже указаны. Обучение будет проходить на приведенном наборе данных.

## Внедрение

Предположительно планируется создать сервис на FAST API с одной-двумя ручками. Первая для healthcheck, а вторая для получения инференса модели. Возможно также нужно будет прикрутить БД, но кажется, что это будет лишним.